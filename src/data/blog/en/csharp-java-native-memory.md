---
author: Ulala-X
pubDatetime: 2025-12-23T00:00:00+09:00
title: C# vs Java Native Memory Handling - The Decisive Difference Between Pin and Copy
slug: csharp-java-native-memory
featured: true
draft: false
lang: en
lang_ref: csharp-java-native-memory
tags:
  - csharp
  - java
  - native-memory
  - performance
  - benchmark
description: Comparing C#'s Zero-copy Pinning with Java's Mandatory Copy Mechanism
---

# C# vs Java Native Memory Handling - The Decisive Difference Between Pin and Copy

> Comparing C#'s Zero-copy Pinning with Java's Mandatory Copy Mechanism

**December 23, 2025**

---

## Why This Difference Matters

When communicating with native libraries or handling high-performance I/O, data transfer between managed memory and native memory is unavoidable. C# and Java use fundamentally different mechanisms:

- **C#**: Can pass managed arrays to native code via Zero-copy using the `fixed` keyword
- **Java**: No pinning capability - Heap â†’ Native copying is mandatory

This difference directly impacts performance when frequently processing large volumes of data. This article provides a concrete comparison of memory handling approaches in both languages through benchmark data.

---

## Chapter 1: C# Native Memory Handling

C# provides several methods for handling native memory. Let's examine the performance characteristics of each through benchmarks.

### 1.1 Native Memory Allocation Methods

**Test Environment**:
- CPU: Intel Core Ultra 7 265K (20 cores)
- OS: Ubuntu 24.04.3 LTS
- Runtime: .NET 8.0.22
- BenchmarkDotNet v0.14.0

| Method | 64B | 1KB | 64KB | 1MB |
|--------|-----|-----|------|-----|
| StackAlloc | 11.7 Î¼s | 12.1 Î¼s | 30.0 Î¼s | 106.1 Î¼s |
| NativeMemory.Alloc | 63.6 Î¼s | 61.5 Î¼s | 61.2 Î¼s | 61.4 Î¼s |
| Marshal.AllocHGlobal | 74.3 Î¼s | 72.6 Î¼s | 82.8 Î¼s | 86.3 Î¼s |

**Key Insights**:

1. **StackAlloc** is fastest at 64B with 11.7 Î¼s, but performance degrades as size increases (106.1 Î¼s at 1MB).
2. **NativeMemory.Alloc** shows consistent performance regardless of size (approximately 61-63 Î¼s). This demonstrates the stability of heap allocation mechanisms.
3. **Marshal.AllocHGlobal** is a legacy API and slowest across all sizes.

**Practical Guide**: Use StackAlloc for small sizes that can be safely allocated on the stack (< 1KB), otherwise use NativeMemory.Alloc.

**Specific usage examples for each allocation method**:

```csharp
// 1. StackAlloc - Allocate on stack memory
unsafe
{
    byte* buffer = stackalloc byte[1024];
    // Automatically freed when function exits
}

// 2. NativeMemory.Alloc - Allocate on native heap (.NET 6+)
unsafe
{
    byte* buffer = (byte*)NativeMemory.Alloc(1024);
    try {
        // Use
    } finally {
        NativeMemory.Free(buffer);
    }
}

// 3. Marshal.AllocHGlobal - Allocate on native heap (legacy)
IntPtr buffer = Marshal.AllocHGlobal(1024);
try {
    // Use
} finally {
    Marshal.FreeHGlobal(buffer);
}
```

**Memory allocation method characteristics comparison**:

| Method | Location | GC Managed | Deallocation | Characteristics |
|--------|----------|------------|--------------|-----------------|
| `stackalloc` | Stack | âŒ | Automatic (scope exit) | Fastest, size limited (~1MB) |
| `NativeMemory.Alloc` | Native Heap | âŒ | Manual (`Free`) | Consistent performance, modern API |
| `Marshal.AllocHGlobal` | Native Heap | âŒ | Manual (`FreeHGlobal`) | Legacy, P/Invoke compatible |
| `new byte[]` | Managed Heap | âœ… | GC | Safest, GC overhead |

**C# process memory structure**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Process Memory                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Stack         â”‚  Managed Heap   â”‚     Native Heap         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ stackalloc    â”‚ â€¢ new byte[]    â”‚ â€¢ NativeMemory.Alloc    â”‚
â”‚ â€¢ Local vars    â”‚ â€¢ GC managed    â”‚ â€¢ Marshal.AllocHGlobal  â”‚
â”‚ â€¢ Auto freed    â”‚ â€¢ Auto freed    â”‚ â€¢ Manual free required  â”‚
â”‚ â€¢ ~1MB limit    â”‚ â€¢ No size limit â”‚ â€¢ No size limit         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 C#'s Core Strength: Zero-copy Pinning

C#'s most powerful feature is Zero-copy transfer using the `fixed` keyword.

```csharp
byte[] managedArray = new byte[1024];

// Pin with fixed to prevent GC from moving memory
unsafe
{
    fixed (byte* ptr = managedArray)
    {
        // Pass pointer to native code without copying
        NativeLibrary.ProcessData(ptr, managedArray.Length);
    }
}
// Automatically unpinned when exiting fixed block
```

**Pinning Benchmark** (10,000 iterations):

| Method | 64B | 1KB | 64KB | 1MB |
|--------|-----|-----|------|-----|
| Fixed (pin only) | 5.8 Î¼s | 5.8 Î¼s | 26.4 Î¼s | 28.9 Î¼s |
| GCHandle.Pinned | 245.4 Î¼s | 245.5 Î¼s | 260.5 Î¼s | 259.8 Î¼s |
| MemoryMarshal (Span) | 4.2 Î¼s | 4.3 Î¼s | 4.5 Î¼s | 4.6 Î¼s |

**Key Insights**:

1. **MemoryMarshal.GetReference** with Span is fastest across all sizes at approximately 4.2-4.6 Î¼s.
2. **fixed** keyword shows respectable performance at 5.8 Î¼s for 64B~1KB.
3. **GCHandle.Pinned** is significantly slower at 245 Î¼s. Explicit allocation/deallocation overhead is substantial.

### 1.3 Managed â†’ Native Data Transfer Performance

We compared methods for transferring data from managed arrays to native memory.

**Benchmark Results** (10,000 iterations):

| Method | 64B | 1KB | 64KB | 1MB |
|--------|-----|-----|------|-----|
| Marshal.Copy | 11.2 Î¼s | 13.8 Î¼s | 106.1 Î¼s | 1,330 Î¼s |
| Buffer.MemoryCopy (fixed) | 12.7 Î¼s | 16.1 Î¼s | 176.1 Î¼s | 2,146 Î¼s |
| Span.CopyTo (native) | 11.2 Î¼s | 15.8 Î¼s | 166.4 Î¼s | 2,104 Î¼s |

**Key Insights**:

1. **For small sizes (64B)**, all methods are similar (approximately 11-13 Î¼s).
2. **For 1MB data**, Marshal.Copy is fastest at 1,330 Î¼s, while Buffer.MemoryCopy is 1.6Ã— slower at 2,146 Î¼s.
3. **If copying is necessary**, Marshal.Copy is the fastest. However, for read-only operations, pinning with fixed for zero-copy transfer as shown in Section 1.2 is optimal.

**Specific usage for each copy method**:

```csharp
byte[] managedArray = new byte[1024];
IntPtr nativePtr = Marshal.AllocHGlobal(1024);

// 1. Marshal.Copy - Managed â†’ Native copy
Marshal.Copy(managedArray, 0, nativePtr, 1024);

// 2. Buffer.MemoryCopy - Copy after pinning with fixed
unsafe
{
    fixed (byte* src = managedArray)
    {
        Buffer.MemoryCopy(src, (void*)nativePtr, 1024, 1024);
    }
}

// 3. Span.CopyTo - Span-based copy
unsafe
{
    Span<byte> nativeSpan = new Span<byte>((void*)nativePtr, 1024);
    managedArray.AsSpan().CopyTo(nativeSpan);
}
```

**Internal operation comparison for copy methods**:

| Method | Internal Operation | Pinning Required | Advantages | Disadvantages |
|--------|-------------------|------------------|------------|---------------|
| `Marshal.Copy` | Internally pin + memcpy | Automatic | Fastest, simple | No unsafe needed |
| `Buffer.MemoryCopy` | Direct memcpy call | Manual (fixed) | Flexible | Explicit pin required |
| `Span.CopyTo` | Internally memmove | Manual (fixed) | Modern API | Slightly slower |

**Memory usage pattern comparison between copy and zero-copy methods**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Copy Method] Marshal.Copy / Buffer.MemoryCopy               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Managed Heap          memcpy           Native Heap        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ byte[]   â”‚      Data copied          â”‚ IntPtr   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                             â”‚
â”‚   â†’ 2Ã— memory usage, copy time required                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Zero-copy] fixed keyword                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Managed Heap                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚ byte[]   â”‚ â†â”€â”€ Pinned with fixed (GC cannot move)        â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚       â”‚                                                     â”‚
â”‚       â””â”€â”€â†’ Pointer directly passed to native code           â”‚
â”‚                                                             â”‚
â”‚   â†’ 1Ã— memory usage, zero copy time                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Practical Application**:
```csharp
// Read-only operations: Zero-copy Pinning
unsafe
{
    fixed (byte* ptr = managedArray)
    {
        NativeLib.Read(ptr, length); // No copying
    }
}

// Write operations: Copying required
IntPtr nativePtr = Marshal.AllocHGlobal(size);
Marshal.Copy(managedArray, 0, nativePtr, size);
NativeLib.Write(nativePtr, size);
Marshal.FreeHGlobal(nativePtr);
```

---

## Chapter 2: Java Native Memory Handling

Since JDK 14, Java has improved native memory management through the Foreign Memory API (now Foreign Function & Memory API). However, there's a critical constraint: **Heap arrays cannot be pinned**.

### 2.1 Arena-based Memory Allocation

**Test Environment**:
- Java: OpenJDK 22.0.2
- JMH 1.37
- OS: Ubuntu 24.04.3 LTS
- CPU: Intel Core Ultra 7 265K

| Arena Type | 64B | 1KB | 64KB | 1MB |
|-----------|-----|-----|------|-----|
| Confined (thread-local) | 42 ns | 47 ns | 887 ns | 13,400 ns |
| Shared (thread-safe) | 31,628 ns | 32,578 ns | 34,144 ns | 53,592 ns |
| Global (permanent) | 88 ns | 630 ns | 35,520 ns | 559,392 ns |
| Auto (automatic) | 482 ns | 818 ns | 37,572 ns | 635,644 ns |

**Key Insights**:

1. **Confined Arena** is fastest across all sizes because it's single-threaded only.
2. **Shared Arena** is very slow at approximately 31 Î¼s for small sizes due to synchronization overhead.
3. **For large sizes (1MB)**, all types show similar performance (approximately 13-635 Î¼s).

**Comparison with C#**:
- C# NativeMemory.Alloc: 61-63 Î¼s (size-independent)
- Java Confined Arena: 42 ns ~ 13.4 Î¼s (size-dependent)

Java is faster for small sizes, but performance drops dramatically when using Shared Arena for thread safety.

**Usage examples for each Arena type**:

```java
// 1. Confined Arena - Single thread only, fastest
try (Arena arena = Arena.ofConfined()) {
    MemorySegment segment = arena.allocate(1024);
    // Only usable in this thread
} // Automatically freed

// 2. Shared Arena - Thread-safe
try (Arena arena = Arena.ofShared()) {
    MemorySegment segment = arena.allocate(1024);
    // Shareable across threads
} // Automatically freed

// 3. Global Arena - Permanent allocation
MemorySegment segment = Arena.global().allocate(1024);
// Persists until process termination, cannot be freed

// 4. Auto Arena - GC-based automatic management
Arena arena = Arena.ofAuto();
MemorySegment segment = arena.allocate(1024);
// No close() needed, GC manages
```

**Arena type characteristics comparison**:

| Arena Type | Thread-Safe | Deallocation | Performance | Use Case |
|-----------|-------------|--------------|-------------|----------|
| `Confined` | âŒ Single thread | try-with-resources | ğŸš€ Fastest | Local operations |
| `Shared` | âœ… Multi-threaded | try-with-resources | ğŸ¢ Sync overhead | Cross-thread sharing |
| `Global` | âœ… Multi-threaded | Cannot free | âš¡ Fast | Constants, global data |
| `Auto` | âœ… Multi-threaded | GC automatic | ğŸ”„ GC dependent | Unclear lifetime |

**Java process memory structure**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      JVM Process Memory                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Java Heap       â”‚           Native Memory               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ byte[] arrays     â”‚ â€¢ MemorySegment (Arena)               â”‚
â”‚ â€¢ Object instances  â”‚ â€¢ DirectByteBuffer internal buffer    â”‚
â”‚ â€¢ GC managed        â”‚ â€¢ JNI native allocation               â”‚
â”‚                     â”‚ â€¢ Released via Arena.close()          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš ï¸ Java cannot pin Heap arrays â†’ Copy mandatory for Native â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Java's Fundamental Constraint: Mandatory Heap â†’ Native Copying

Java cannot pin heap arrays. Therefore, **copying is mandatory for all heap data transfer to native**.

**Heap â†’ Native Copy Benchmark**:

| Method | 64B | 1KB | 64KB | 1MB |
|--------|-----|-----|------|-----|
| Heap â†’ MemorySegment Copy | 44 ns | 52 ns | 1,825 ns | 39,434 ns |
| Heap â†’ DirectBuffer Copy | 373 ns | 743 ns | 36,175 ns | 553,142 ns |
| Heap â†’ Native (reusable) | 2.5 ns | 8.4 ns | 1,390 ns | 25,074 ns |

**Key Insights**:

1. **Using reusable buffers** significantly reduces copy overhead (25 Î¼s at 1MB).
2. **DirectBuffer** is very slow when allocated each time (553 Î¼s at 1MB).
3. **MemorySegment** is generally faster than DirectBuffer.

**Decisive Difference from C#**:

```java
// Java: Copying mandatory
byte[] heapArray = new byte[1024];
try (Arena arena = Arena.ofConfined()) {
    MemorySegment segment = arena.allocate(1024);
    MemorySegment.copy(heapArray, 0, segment, 0, 1024); // Copy occurs
    nativeProcess(segment);
}
```

```csharp
// C#: Zero-copy possible
byte[] managedArray = new byte[1024];
unsafe {
    fixed (byte* ptr = managedArray) {
        NativeProcess(ptr); // No copying
    }
}
```

### 2.3 DirectByteBuffer vs MemorySegment

**Allocation Performance**:

| Method | 64B | 1KB | 64KB | 1MB |
|--------|-----|-----|------|-----|
| DirectByteBuffer.allocate | 367 ns | 708 ns | 33,489 ns | 578,204 ns |
| MemorySegment.allocate | 43 ns | 50 ns | 930 ns | 13,480 ns |

**MemorySegment is 42Ã— faster than DirectByteBuffer** (at 1MB).

**Practical Guide**:
- Use MemorySegment instead of DirectByteBuffer unless working with legacy code
- Reuse Arena to minimize allocation overhead for frequent allocations

---

## Chapter 3: Pin vs Copy - Core Differences

### 3.1 Architectural-Level Differences

| Feature | C# | Java |
|---------|-----|------|
| **Pinning Support** | âœ… `fixed`, `GCHandle` | âŒ Not possible |
| **Zero-copy Transfer** | âœ… Managed â†’ Native direct | âŒ Copying mandatory |
| **GC Impact** | Memory immovable during pinning | Independent of GC (copies) |
| **Performance** | Zero copy cost | Copy cost proportional to size |
| **Safety** | Requires unsafe blocks | Type-safe |

### 3.2 Performance Impact Comparison

**Cost of transferring 1MB data to native**:

| Language | Method | Time | Notes |
|----------|--------|------|-------|
| C# | Fixed (Zero-copy) | ~29 Î¼s | Pinning only |
| C# | Marshal.Copy | 1,330 Î¼s | With copying |
| Java | Heap â†’ MemorySegment | 39,434 ns (39.4 Î¼s) | Copying mandatory |
| Java | Reusable buffer | 25,074 ns (25.1 Î¼s) | Optimized copying |

**Key Insights**:

1. **C# Zero-copy**: 29 Î¼s (fastest)
2. **Java optimized copying**: 25.1 Î¼s (comparable to C#)
3. **C# Marshal.Copy**: 1,330 Î¼s (slowest)

Interestingly, Java's reusable buffer copying is 53Ã— faster than C#'s Marshal.Copy. However, C# can avoid copying altogether with fixed.

### 3.3 Large Data Processing Scenario

**Scenario**: Transfer 64MB data to native 1,000 times

| Language | Method | Total Time (Estimated) |
|----------|--------|------------------------|
| C# | Fixed (Zero-copy) | ~29 ms |
| Java | Reusable buffer copy | ~25.1 ms |
| Java | Allocate/copy each time | ~39.4 ms |
| C# | Marshal.Copy | ~1,330 ms |

**Practical Implications**:
- If the native library uses data read-only, C#'s fixed is optimal.
- For Java, reusing buffers is essential. Allocating each time is 57% slower.

---

## Chapter 4: Practical Application Guide

### 4.1 C# Native Memory Usage Patterns

**Pattern 1: Passing Read-only Data to Native**

```csharp
// Pass to native via Zero-copy
byte[] imageData = LoadImage();

unsafe
{
    fixed (byte* ptr = imageData)
    {
        ProcessImageNative(ptr, imageData.Length);
    }
}
```

**Use Cases**:
- Image/video processing (OpenCV, FFmpeg, etc.)
- Cryptography libraries (OpenSSL, etc.)
- Network packet processing

**Pattern 2: Writable Native Buffer**

```csharp
// Allocate native buffer with NativeMemory.Alloc
unsafe
{
    byte* nativeBuffer = (byte*)NativeMemory.Alloc(1024);

    try
    {
        FillDataNative(nativeBuffer, 1024);

        // Copy result to managed
        byte[] result = new byte[1024];
        fixed (byte* dest = result)
        {
            Buffer.MemoryCopy(nativeBuffer, dest, 1024, 1024);
        }
    }
    finally
    {
        NativeMemory.Free(nativeBuffer);
    }
}
```

**Pattern 3: High-performance I/O with Span**

```csharp
// Zero-copy read/write with Span
Span<byte> buffer = stackalloc byte[512];

int bytesRead = socket.Receive(buffer);

unsafe
{
    fixed (byte* ptr = buffer)
    {
        ProcessData(ptr, bytesRead);
    }
}
```

### 4.2 Java Native Memory Usage Patterns

**Pattern 1: Minimize Allocation by Reusing Arena**

```java
// Pre-create and reuse Arena
private final Arena arena = Arena.ofShared();
private final MemorySegment reusableBuffer =
    arena.allocate(1024 * 1024); // 1MB buffer

public void processData(byte[] heapData) {
    // Copy to reusable buffer
    MemorySegment.copy(heapData, 0,
        reusableBuffer, ValueLayout.JAVA_BYTE, 0, heapData.length);

    nativeProcess(reusableBuffer);
}

// On shutdown
public void close() {
    arena.close();
}
```

**Pattern 2: Streaming Data Processing**

```java
// Process in chunks for memory efficiency
try (Arena arena = Arena.ofConfined()) {
    MemorySegment chunk = arena.allocate(4096);

    while (hasMoreData()) {
        byte[] heapChunk = readChunk();
        MemorySegment.copy(heapChunk, 0, chunk, 0, heapChunk.length);
        processChunk(chunk);
    }
}
```

**Pattern 3: DirectByteBuffer Legacy Code**

```java
// Integration with legacy NIO code
ByteBuffer directBuffer = ByteBuffer.allocateDirect(1024);
directBuffer.put(heapArray);
directBuffer.flip();

// Wrap as MemorySegment (Zero-copy)
MemorySegment segment = MemorySegment.ofBuffer(directBuffer);
nativeProcess(segment.address());
```

### 4.3 When to Choose Which Language

**Choose C# when**:

1. **Large read-only data processing**
   - Example: Processing 100MB images with native libraries
   - Reason: Zero-copy eliminates copy costs

2. **Frequent Managed â†” Native transitions**
   - Example: 10,000 native function calls per second
   - Reason: Minimal overhead with fixed keyword

3. **Memory efficiency is critical**
   - Example: IoT devices, embedded systems
   - Reason: Memory reuse without copying

**Choose Java when**:

1. **Safety is paramount**
   - Example: Financial systems, medical systems
   - Reason: Type-safe API without unsafe

2. **Cross-platform consistency**
   - Example: Identical code on Linux/macOS/Windows
   - Reason: JVM abstracts platform differences

3. **Small data processing**
   - Example: Frequent native calls with < 1KB data
   - Reason: Copy overhead is negligible (< 100ns)

---

## Conclusion

### Key Summary

1. **C#'s Strength: Zero-copy Pinning**
   - Direct transfer of managed arrays to native with `fixed` keyword
   - Zero copy cost for large data
   - Requires unsafe code

2. **Java's Constraint: Mandatory Copying**
   - Cannot pin heap arrays
   - All data must be copied to native
   - Overhead minimizable through Arena reuse

3. **Performance Differences**
   - Small sizes (< 1KB): Both languages similar (< 100ns difference)
   - Large sizes (> 64KB): C# Zero-copy advantage
   - Optimized Java copying is still fast enough (25Î¼s for 1MB)

4. **Practical Selection Criteria**
   - Read-only large data: C# advantage
   - Safety and type safety: Java advantage
   - Extreme performance optimization: C# advantage
   - Cross-platform consistency: Java advantage

### Final Recommendations

**For C# Developers**:
- Use `fixed` or `Span<T>` for Zero-copy when possible
- Small sizes: `stackalloc`, large sizes: `NativeMemory.Alloc`
- Use `Marshal.AllocHGlobal` only for legacy code

**For Java Developers**:
- Use MemorySegment instead of DirectByteBuffer unless legacy
- Reuse Arena to minimize allocation overhead
- Process large data in chunks for memory efficiency

Both languages provide modern and powerful native memory APIs. The key is understanding each language's characteristics and selecting the optimal pattern for your use case.

---

**Related Resources**:
- [C# NativeMemory API Documentation](https://learn.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.nativememory)
- [Java Foreign Function & Memory API](https://openjdk.org/jeps/454)
- [Benchmark Source Code](https://github.com/ulala-x/ulala-x.github.io/tree/main/project/csharp-java-memory)
